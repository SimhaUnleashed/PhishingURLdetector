{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phishing Site Detector\n",
    "\n",
    "Dataset from https://www.kaggle.com/taruntiwarihp/phishing-site-urls\n",
    "\n",
    "The task is to create a model which returns if the given URL is phishing site or not.\n",
    "\n",
    "Place the dataset and this notebook in the same folder to run without any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data= pd.read_csv(\"phishing_site_urls.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x=\"Label\",data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We need to remove all special characters from the url and convert into a list of words. We can do this using regextokenizer\n",
    "* We need to take the processed string and convert into English words\n",
    "* When we do the above steps, the words will be separated with commas. This will make the process of converting into numbers harder.\n",
    "* So we shall remove the commas and make it more like a sentence (to humans this sentence may not make any sense but machines can make sense )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove special characters using regex and convert them into tokens (aka list of words )\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "tokenizer = RegexpTokenizer(r'[A-Za-z]+')\n",
    "data['tokens'] = data.URL.map(lambda t: tokenizer.tokenize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "data['english_words'] = data['tokens'].map(lambda l: [stemmer.stem(word) for word in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Sidenote: Lambda functions are one liner functions which can be used to do any task.\n",
    " Instead of using def keyword to define a function we can use lambda to make one liners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentences'] = data['english_words'].map(lambda l: ' '.join(l))\n",
    "# So basically we are replacing commas with empty spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall convert the sentences column to vectors (aka the format which our model can understand and interpret) and we shall split the data into training and test sets using train_test_split\n",
    "\n",
    "* CountVectorizer is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "feature = CountVectorizer().fit_transform(data.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feature, data.Label, test_size = 0.15)\n",
    "#split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "np.random.seed(45)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f'The model scores {accuracy*100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.random.seed(45)\n",
    "randomforestmodel = RandomForestClassifier()\n",
    "randomforestmodel.fit(X_train,y_train)\n",
    "accuracy = randomforestmodel.score(X_test, y_test)\n",
    "print(f'The random forest model scores {accuracy*100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "np.random.seed(45)\n",
    "decisionmodel = DecisionTreeClassifier()\n",
    "decisionmodel.fit(X_train,y_train)\n",
    "accuracy = decisionmodel.score(X_test, y_test)\n",
    "print(f'The decision tree model scores {accuracy*100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "np.random.seed(45)\n",
    "svcmodel = SVC()\n",
    "svcmodel.fit(X_train,y_train)\n",
    "accuracy = svcmodel.score(X_test, y_test)\n",
    "print(f'The svc forest model scores {accuracy*100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "np.random.seed(45)\n",
    "multimodel = MultinomialNB()\n",
    "multimodel.fit(X_train,y_train)\n",
    "accuracy = multimodel.score(X_test, y_test)\n",
    "print(f'The multinomial model scores {accuracy*100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "np.random.seed(45)\n",
    "gaussmodel = GaussianNB()\n",
    "gaussmodel.fit(X_train,y_train)\n",
    "accuracy = gaussmodel.score(X_test, y_test)\n",
    "print(f'The gauss model scores {accuracy*100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "np.random.seed(45)\n",
    "knmodel = KNeighborsClassifier()\n",
    "knmodel.fit(X_train,y_train)\n",
    "accuracy = knmodel.score(X_test, y_test)\n",
    "print(f'The knn model scores {accuracy*100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "np.random.seed(45)\n",
    "gbmodel = GradientBoostingClassifier()\n",
    "gbmodel.fit(X_train,y_train)\n",
    "accuracy = gbmodel.score(X_test, y_test)\n",
    "print(f'The gradient boosting model scores {accuracy*100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new  = 'https://dlscordapp.codes/billing/promotions/rJSuZk5ySk6Sf6qnk4v9bHEG/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "api_pipeline = make_pipeline(CountVectorizer(tokenizer = RegexpTokenizer(r'[A-Za-z]+').tokenize,stop_words='english'), LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(data.URL, data.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_pipeline.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_pipeline.score(testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(api_pipeline,open('phishing.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('phishing.pkl', 'rb'))\n",
    "result = loaded_model.predict(['https://www.linkedin.com'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = loaded_model.predict(['https://dlscordapp.codes/billing/promotions/rJSuZk5ySk6Sf6qnk4v9bHEG/'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
